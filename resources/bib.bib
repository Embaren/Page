@inproceedings{barbierrenard_reconstruction_2025,
  title = {Reconstruction 3D depuis des couples SAR ascendant/descendant},
  author = {Barbier--Renard, Emile and Tupin, Florence and Denis, Loïc},
  booktitle = {GRETSI},
  address = {Strasbourg, France},
  year = {2025},
  month = {Aug},
}

@article{barbierrenard_multiview_2025,
	title = {Multiview 3-{D} {Surface} {Reconstruction} {From} {SAR} {Images} by {Inverse} {Rendering}},
	volume = {22},
	issn = {1558-0571},
	url = {https://ieeexplore.ieee.org/document/11008740},
	doi = {10.1109/LGRS.2025.3572303},
	abstract = {The 3-D reconstruction of a scene from synthetic aperture radar (SAR) images mainly relies on interferometric measurements, which involve strict constraints on the acquisition process. These last years, progress in deep learning has significantly advanced 3-D reconstruction from multiple views in optical imaging, mainly through reconstruction-by-synthesis approaches popularized by neural radiance fields (NeRFs). In this article, we propose a new inverse rendering method for 3-D reconstruction from a few incoherent SAR views, drawing inspiration from optical approaches. First, we introduce a new simplified differentiable SAR rendering model, able to synthetize images from a digital surface model (DSM) and a radar backscattering coefficients map. Then, we introduce a coarse-to-fine strategy to reconstruct the DSM and the map of backscattering coefficients of an SAR scene starting only from a few SAR views. We use a neural field, i.e., a continuous parametric model based on a multilayer perceptron, to represent the SAR scene. Finally, we present preliminary results of DSM reconstruction from synthetic SAR images produced by ONERA’s physically based EMPRISE simulator, supporting the potential of applying inverse rendering approaches to SAR data to efficiently exploit geometric disparities in future applications such as multisensor data fusion.},
	urldate = {2025-09-12},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Barbier–-Renard, Emile and Tupin, Florence and Trouvé, Nicolas and Denis, Loïc},
	year = {2025},
	keywords = {Backscatter, Deep learning, Image reconstruction, inverse rendering, Neural radiance field, neural radiance fields (NeRFs), Radar, Radar polarimetry, Rendering (computer graphics), SAR image simulation, Solid modeling, surface reconstruction, Surface reconstruction, Synthetic aperture radar, synthetic aperture radar (SAR), Three-dimensional displays},
	pages = {1--5},
}

@inproceedings{noyel_logarithmic_2022,
	address = {Cham},
	title = {Logarithmic {Morphological} {Neural} {Nets} {Robust} to {Lighting} {Variations}},
	isbn = {978-3-031-19897-7},
	doi = {10.1007/978-3-031-19897-7_36},
	abstract = {Morphological neural networks allow to learn the weights of a structuring function knowing the desired output image. However, those networks are not intrinsically robust to lighting variations in images with an optical cause, such as a change of light intensity. In this paper, we introduce a morphological neural network which possesses such a robustness to lighting variations. It is based on the recent framework of Logarithmic Mathematical Morphology (LMM), i.e. Mathematical Morphology defined with the Logarithmic Image Processing (LIP) model. This model has a LIP additive law which simulates in images a variation of the light intensity. We especially learn the structuring function of a LMM operator robust to those variations, namely: the map of LIP-additive Asplund distances. Results in images show that our neural network verifies the required property.},
	language = {en},
	booktitle = {Discrete {Geometry} and {Mathematical} {Morphology}},
	publisher = {Springer International Publishing},
	author = {Noyel, Guillaume and Barbier--Renard, Emile and Jourlin, Michel and Fournel, Thierry},
	editor = {Baudrier, Étienne and Naegel, Benoît and Krähenbühl, Adrien and Tajine, Mohamed},
	year = {2022},
	keywords = {Functional Asplund metric, Logarithmic Image Processing, Logarithmic Mathematical Morphology, Morphological neural nets, Robustness to lighting variations},
	pages = {462--474},
}
